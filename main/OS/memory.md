
> CPU는 그저 **메모리**에 올라와 있는 프로그램의 명령어를 실행할 뿐.

# 3.2.1 메모리계층

![image](https://github.com/cJinu/CS/assets/94429120/0e0b8dd5-d4e6-4b73-91a8-13984ce6e747)
메모리 계층은 레지스터, 캐시, 메모리, 저장장치로 구성되어 있다.

- 레지스터: CPU 안에 있는 작은 메모리, 휘발성, 속도 빠름, 기억용량적음
- 캐시: L1, L2 캐시, 휘발성, 속도빠름, 기억용량 적음
- 주기억장치: RAM, 휘발성, 속도 보통, 기억용량보통
- 보조기억장치: HDD, SSD, 비휘발성, 속도 느림, 기억용량많음
<br>
<br>

## RAM
주기억장치인 RAM은 보조기억장치인 하드디스크로 부터 일정량의 데이터를 복사해서 임시 저장하고 이를 필요 시마다 CPU에 **빠르게** 전달하는 역할을 합니다. <br>
위의 사진을 보면 계층이 올라갈수록 용량은 작고 가격은 비싼 특징을 보인다. <br>
이런 계층을 보이는 이유는 경제성과 캐시 때문이다. <br>
16GB 램 - 80,000 하지만 16GB SSD는 훨씬 싼 가격에 구할 수 있습니다 

## 캐시
캐시(cache)란 데이터를 미리 복사해 놓은 임시 저장소이자 상위 계층과 하위 계층의 속도 차이로 인한 병목 현상을 줄이기 위한 메모리 <br>
실제로 메모리와 CPU 사이의 속도 차이가 크기 때문에 중간에 레지스터 계층을 둬서 속도 차이를 해결합니다. 이렇게 계층과 계층 사이에 있는 계층을 캐싱 계층이라고 한다. <br>
예를 들어 캐시 계층과 로컬 저장장치(보조기억장치 HDD)에 있는 메인 메모리(주기억장치 RAM)을 로컬 저장장치의 캐싱 계층이라고 할 수 있다.

## 지역성
캐시를 직접 설정할 때는 자주 사용하는 데이터를 기반으로 설정해야 한다. <br>
이렇게 사용되는 것이 시간 지역성과 공간 지역성이다.

- ### 시간 지역성
최근에 사용한 데이터에 다시 접근하려는 특성 <br>
대표적인 예시로 for문의 int i에 대해서 계속해서 접근해서 i++을 해준다.<br>
i에 계속해서 접근했을 때와 i에 접근을 하지만 실제로는 j에 할당할때의 차이점을 보자. <br>

![image](https://github.com/cJinu/CS/assets/94429120/f249a9fa-1c7e-4045-8ec5-8ba008eb3f80)

![image](https://github.com/cJinu/CS/assets/94429120/64c9a4c5-996d-4ecb-b2f5-a790d87ab540)

이 경우에는 약 8배나 차이가 난다는것을 알 수 있다.

- ### 공간지역성
공간적 지역성은 기억장치 내에 서로 인접하여 저장되어 있는 데이터들이 연속적으로 액세스 될 가능성이 높아지는 특성이다.

CPU 캐시나 디스크 캐시의 경우 한 메모리 주소에 접근할 때 그 주소뿐 아니라 해당 블록을 전부 캐시에 가져오게 된다.

이때 메모리 주소를 오름차순이나 내림차순으로 접근한다면, 캐시에 이미 저장된 같은 블록의 데이터를 접근하게 되므로 캐시의 효율성이 크게 향상된다.

위의 예시로 예를 들자면 한번 접근했던 배열 matrix에 계속 접근하는 것을 알 수 있다.

## 캐시히트와 캐시미스
캐시가 메모리에 있는 데이터를 미리 모아두고 있을 때 제어장치에서 원하는 데이터가 캐시에 있다면 캐시 히트, 해당 데이터가 없다면 캐시에 없어서 메모리에 접근하는 경우를 캐시 미스라고 부른다.

![image](https://github.com/cJinu/CS/assets/94429120/7ad02b66-8d66-46d7-acb8-aebf85756ad2)

이렇게 캐시히트가 하게 된다면 CPU 내부 버스를 기반으로 작동하기 때문에 빠르게 데이터를 가져올 수 있는 반면에 캐시미스가 발생하면 시스템 버스를 기반으로 작동되기 때문에 데이터 전송이 느리다.

## 캐시매핑
캐시매핑이란 캐시가 히트되기 위하는 방법을 의미하며 CPU의 레지스터와 RAM 간의 데이터를 주고받을 때를 기반으로 설명한다. <br>
레지스터는 RAM에 비해 매우 작고 RAM은 레지스터에 비해 매우 크기 때문에 이런 레지스터를 잘 활용하려면 캐시매핑을 잘 설정하는것이 중요하다.

- 직접매핑 <br>
메모리 주소와 캐시의 순서를 일치시킨다. 메모리가 1 ~ 100까지 있고 캐시가 1 ~ 10까지 있다면 1 ~ 10까지의 메모리는 캐시의 1에 위치하고 11~20까지의 메모리는 캐시의 2에 위치시키는 것이다. 구현이 정말 간단하지만 저 규칙을 만족시켜서 캐시를 넣다 보면 캐시가 효율적이지 않게 자꾸 교체되어야 하는 일이 생긴다. 

- 연관매핑 <br>
순서를 일치시키지 않는다. 필요한 메모리값을 캐시의 어디든 편하게 저장 될 수 있다. 당연히 찾는 과정은 복잡하고 느릴 수 있지만 정말 필요한 캐시들 위주로 저장할 수 있기 때문에 적중률은 높다. 캐시가 일반 메모리보다 속도가 훨씬 빠르므로 캐시의 검색량을 신경쓰는 것 보단 적중률이 높은게 성능이 더 좋다.

- 집합연관매핑 <br>
연관매핑에 직접매핑을 합쳐 놓은 방식이다. 순서를 일치시키고 편하게 저장하되, 일정 그룹을 두어 그 그룹 내에서 편하게 저장시키는 것이다. 검색은 좀 더 효율적으로 되고 직접매핑처럼 저장위치에 대한 큰 제약이 있는건 아니기 때문에 적중률이 많이 떨어지지도 않는다. 


| 직접매핑 | 연관 매핑 | 집합 연관 매핑 |
| --- | --- | --- |
| 처리가 빠르지만 충돌이 잦음 | 충돌이 적지만 속도가 느림 | 블록화하여 검색은 효율적|

## 웹브라우저의 캐시
대표적인 캐시로는 쿠키, 로컬 - 세션 스토리지가 있다. <br>
보통 사용자의 커스텀한 정보나 인증 모듈 관련 사항을 웹 브라우저에 저장해서 자신을 증명하거나 중복 요청 방지를 위해 사용됨

- 쿠키 <br>
 쿠키(웹 쿠키, 브라우저 쿠키)는 서버가 사용자의 웹 브라우저에 전송하는 작은 데이터 조각입니다. 브라우저는 그 데이터 조각들을 저장해 놓았다가, 동일한 서버에 재 요청 시 저장된 데이터를 함께 전송합니다. 쿠키는 두 요청이 동일한 브라우저에서 들어왔는지 아닌지를 판단할 때 주로 사용합니다. 이를 이용하면 사용자의 로그인 상태를 유지할 수 있습니다. 상태가 없는(stateless) HTTP 프로토콜에서 상태 정보를 기억시켜주기 때문입니다. <br>
 쿠키를 설정할 때 조심해야할 점이 document.cookie를 통해서 클라이언트가 쿠키를 확인할 수 없게 httponly 옵션을 걸어야한다. 쿠키의 만료기간은 보통 서버에서 정한다.

 - 로컬 스토리지 <br>
 만료기간이 없는 키-값 저장소 <br>
 브라우저를 닫아도 유지되며 도메인 단위로 저장, 생성이 되며 HTTP5를 지원하지 않는 경우에는 사용할 수 없으며 클라이언트에서만 수정이 가능하다.

- 세션 스토리지 <br>
 만료기간이 없는 키-값 저장소 <br>
 탭 단위로 세션 스토리지를 생성하며, 탭을 닫을때는 삭제되며 HTTP5를 지원하지 않는 경우에는 사용할 수 없으며 클라이언트에서만 수정이 가능하다.


 > 로컬스토리와 세션 스토리지의 차이점 <br>
 > 로컬스토리지는 탭을 닫아도 유지, 세션 스토리지는 탭을 닫으면 삭제!

 ## 데이터베이스의 캐싱 계층
 앞에서 설명했던것과 동일하게 데이터베이스 또한 **REDIS** 라는 데이터베이스 계층을 캐싱계층으로 사용하여 성능을 향상시킨다. <br>
 사용자가 늘어나면 DB에 무리가 가기 시작한다. DB는 데이터를 물리 디스크에 직접 쓰기 때문에 서버에 문제가 발생해도 데이터가 손실되지는 않지만, 매 트랜잭션마다 디스크에 접근해야하므로 부하가 많아지면 성능이 떨어진다.

![image](https://github.com/cJinu/CS/assets/94429120/50aca343-1b4b-429d-93ed-bac8698e0fd9)


## REDIS란?
Redis는 오픈 소스로서 **NoSQL**로 분류되기도 하고, 다양한 데이터 구조체를 지원함으로써 DB, Cache, Message Queue, Shared Memory 용도로 사용될 수 있다.<br>
한편, Redis는 Remote Dictionary Server의 약자로 외부에서 사용 가능한 Key-Value 쌍의 해시 맵 형태의 서버라고 생각할 수 있다. 그래서 별도의 쿼리 없이 Key를 통해 빠르게 결과를 가져올 수 있다.



![image](https://github.com/cJinu/CS/assets/94429120/32afcc2c-9437-439f-a599-ab8ce2c1b3c6)

**즉 REDIS란  고성능 키-값 저장소로서 String, list, hash, set, sorted set 등의 자료 구조를 지원하는 NoSQL이다.**
![image](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6416f30d-bcfb-4f84-bdec-82fb69a73f43%2FUntitled.png?table=block&id=d56c51ce-1c17-47f7-b92f-624e875adf08&spaceId=b453bd85-cb15-44b5-bf2e-580aeda8074e&width=2000&userId=80352c12-65a4-4562-9a36-2179ed0dfffb&cache=v2)
- NoSQL이란?<br>
**NoSQL은 비관계형 데이터베이스로 SQL, 즉, 관계형 데이터베이스를 제외한 나머지 유형을 말한다.**<br>
 NoSQL 데이터베이스는 테이블 형식이 아니며, 관계형 테이블과는 다른 방식으로 데이터를 저장한다. <br>
 데이터 유형에 따라 다양한 유형을 갖추고 있으며, 주요 유형으로는 문서, 키 값, 와이드 컬럼,그래프 등이 있다.<br>
NoSQL이라고 해서 꼭 스키마가 없는 것은 아니다. 유연한 스키마를 제공하며, 대량의 데이터와 높은 사용자 부하에서도 손쉽게 확장할 수 있다는 점이 큰 장점입이다.<br>
또한 데이터를 읽어올 때 스키마에 따라 데이터를 읽어 옵니다.

    -  NoSQL의 장점 <br>
        스키마가 없기 때문에 유연하고 언제든지 저장된 데이터를 조정하고 새로운 필드를 추가할 수 있다.<br>
        데이터는 애플리케이션이 필요로 하는 형식으로 저장되기 때문에 데이터를 읽어오는 속도가 빨라진다.<br>
        수직 및 수평 확장이 가능해서 애플리케이션이 발생시키는 모든 읽기와 쓰기 요청 처리가 가능하다.
        
    - NoSQL의 단점 <br>
        유연성으로 인해 데이터 구조 결정을 미루게 될 수 있다.<br>
        데이터 중복을 계속 업데이트해야 한다.<br>
        데이터가 여러 컬렉션에 중복되어 있기 때문에 수정이 필요한 경우 모든 컬렉션에서 수행해야 한다.


! 캐싱이 중요한 이유
> 파레토법칙! <br>
> 80%의 결과는 20%의 원인으로 인해 발생한다는 뜻이다.
> ![image](https://github.com/cJinu/CS/assets/94429120/6fe86384-d10c-43e8-8352-4e0ef1860f3d)


# 3.2.2 메모리 관리
> 운영체제의 대표적인 할 일 중 하나가 메모리 관리이다. 한정된 메모리를 극한으로 활용하는 방법

## 가상 메모리
메모리 관리 기법의 하나로 컴퓨터가 실제로 이용 가능한 메모리 자원을 추상화하여 이를 사용하는 사용자들에게 매우 큰 메모리로 보이게 만드는 것

![image](https://blog.kakaocdn.net/dn/dlPNot/btqOxsaKEXS/KwlNNjKb7sGsmAlk4ta0Y1/img.png)
- 애플리케이션이 실행될 때, 실행에 필요한 일부분만 메모리에 올라가며 애플리케이션의 나머지는 디스크에 남게 됨. 즉, 디스크가 RAM의 보조 기억장치(backing store)처럼 작동하는 것임.
    - 결국 빠르고 작은 기억장치(RAM)을 크고 느린 기억장치(디스크)와 병합하여, 하나의 크고 빠른 기억장치(가상 메모리)처럼 동작하게 하는 것임.
- 가상 메모리를 구현하기 위해서는 컴퓨터가 특수 메모리 관리 하드웨어를 갖추고 있어야만 함. ⇒ 바로 MMU(Memory Management Unit)!
 
1. MMU는 가상주소를 물리주소로 변환하고, 메모리를 보호하는 기능을 수행함.
2. MMU를 사용하게 되면, CPU가 각 메모리에 접근하기 이전에 메모리 주소 번역 작업이 수행됨.
3. 그러나 메모리를 일일이 가상 주소에서 물리적 주소로 번역하게 되면 작업 부하가 너무 높아지므로, MMU는 RAM을 여러 부분(페이지, pages)로 나누어 각 페이지를 하나의 독립된 항목으로 처리함.
4. 페이지 및 주소 번역 정보를 기억하는 작업이 가상 메모리를 구현하는 데 있어 결정적인 절차임.

이때 가상적으로 주어진 주소를 가상 주소, 실제 메모리에 있는 주소를 실제 주소라고 하며 위에서 설명한 MMU를 통해서 실제 주소와 가상 주소를 변환할 수 있다.

가상 메모리는 가상 주소와 실제 주소가 매핑되어 있는 페이지 테이블로 관리되며 이때 성능 향상을 위해 TLB를 사용합니다.

> TLB란? <br>
> TLB는 가상 메모리 주소를 물리적 주소로 변환하는 속도를 높이기 위해 사용하는 캐시로, 최근에 일어난 가상 메모리와 물리 주소의 변환 테이블을 저장해둠. CPU가 가상 주소를 가지고 메모리에 접근하려고 할 때 우선은 TLB에 접근하여 가상 주소에 해당되는 물리 주소를 찾고, 만약 TLB에 매핑이 존재하지 않는다면 MMU가 페이지 테이블에서 해당되는 물리 주소로 변환한 후 메모리에 접근하게 됨. <br>
> 즉, 물리주소를 갖고 있으면 메모리(RAM)에 두 번 들릴 필요없이, 바로 해당 물리주소(in 메모리)를 찾아갈 수 있음.

## 스와핑
멀티 프로그램, 편의성 등의 이유로 실제 메모리보다 더 많은 메모리가 필요하다
디스크에 있는 모든 프로세스의 페이지를 메모리에 올릴 수 없다. 

- 모든 페이지들은 물리 메모리에 매핑되어야 한다
    - 많은 페이지들이 동시에 사용되어서 낭비다
- 물리 메모리는 한정적이다
    - 현재 사용하는 페이지만 올리는 것이 효율적이다

>현재 사용하지 않는 페이지를 디스크로 Swapping

이런 스와핑을 통해서 페이지 폴트가 일어나지 않은 것처럼 만든다.

## 페이지 폴트
프로세스의 주소 공간에 존재하지만 램에는 없는 데이터에 접근했을 때 발생<br>
페이지 폴트가 발생하면 운영체제는 그 데이터를 메모리로 가져와서, 페이지 폴트가 발생하지 않은 것처럼 작동하게 해줌 <br>
이런 페이지 폴트가 자주 발생할 수록 운영체제의 성능은 저하되기 때문에 페이지 폴트가 일어나지 않도록 하는 것이 중요함

### 페이지 폴트와 그로 인한 스와핑 과정
1. CPU는 물리 메모리를 확인하여 해당 페이지가 없다면 트랩을 발생하여 OS에게 알림
2. OS는 CPU의 동작을 멈춤
3. OS는 페이지 테이블을 확인하여 가상 메모리에 페이지를 존재하는지 확인하고, 없다면 프로세스를 중단 후 현재 물리 메모리에 비어있는 프레임이 있는지 찾는다. 물리 메모리에도 없다면 스와핑이 발동함
4. 비어있는 프레임에 해당 페이지를 로드하고, 페이지 테이블을 최신화 함
5. CPU를 다시 시작함

# 스레싱
메모리에 페이지 폴트 비중이 높다면 발생하는 현상<br>
페이지 폴트가 많이 일어나면 CPU의 이용률이 낮아짐 <br>
그렇기 때문에 CPU는 더 많은 프로세스는 메모리에 올리고 또 다시 페이지 폴트가 발생하는 악순환이 반복되어 스레싱이 일어남

이를 해결하기 위해 작업세트와 PFF가 있음

### 작업 세트
프로세스의 과거 작업 이력은 **지역성**을 이용해서 결정된 페이지 집합을 미리 만들어서 메모리에 로드를 해놓는 작업 <br>
미리 메모리에 로드하면 탐색에 드는 비용도 감소하며, 스와핑도 줄어들게 된다. 즉 페이지 폴트 빈도가 낮아지게 된다.
![image](https://github.com/cJinu/CS/assets/94429120/66b0ecb9-c14f-447d-a76c-2c30364e9423)

### PFF
PFF(Page Fault Frequency)는 페이지 폴트 빈도의 상한선과 하한선을 조정하는 방법 <br>
상한선에 도달하면 프레임을 늘리고, 하한성에 도달하면 프레임을 줄인다.

![image](https://github.com/cJinu/CS/assets/94429120/19323821-5174-4a6a-a707-05332bb43d30)


| 구분 | Working Set | PFF |
| --- | --- | --- |
|페이지 집합 수정 방식 | 매번 기억 장치를 참조한 뒤 수정 | 페이지 폴트 발생시만 수정
| 스레싱 조절 |  로드는 유용하나 조절이 어렵고 추정이 어려움 | 직접적으로 스레싱을 방지하면서 PFF 측정 및 조절
| 오버헤드 | 매우 큼 | 적음 |
<br>

# 메모리 할당
메모리에 프로그램을 할당할 때는 시작 메모리 위치, 메모리의 할당 크기를 기반으로 할당하는데, 연속 할당과 불연속 할당으로 나누어진다.

## 연속 할당

![image](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbf18da9d-c3cb-44ac-9ecb-1b87ad35f0aa%2FUntitled.png&blockId=d6922bba-602d-40ef-bd4b-8486e5ea53d1)

위의 그럼처럼 프로세스가 순차적으로 공간에 할당하는 것

- 고정 분할 방식
물리적 메모리를 주어진 개수만큼의 영구적인 분할로 미리 나누어두고 각 분할에 하나의 프로세스를 적재하는 방식, 메모리가 미리 나누어져 있기 때문에 외부 단편화 또는 내부 단편화가 발생할 수 있다.

    - 외부 단편화: 프로그램의 크기보다 분할의 크기가 작기 때문에 공간이 있어서 적재되지 못하는 현상
    ![image](https://github.com/cJinu/CS/assets/94429120/3a9b1111-fe85-4a6a-95f9-2e073d304ee6)

    - 내부 단편화: 프로그램의 크기보다 분할의 크기가 큰 경우 프로그램을 적재하고 남는 현상
    ![image](https://github.com/cJinu/CS/assets/94429120/7a70853d-7c54-473d-b81b-7121537dc2b1)

- 가변 분할 방식 <br>
매 시점 프로그램의 크기에 맞게 동적으로 메모리는 나눠 사용하는 방식, 내부 단편화는 발생하지 않고 외부 단편화만 발생함 <br>
메모리에 올라가 있는 프로세스가 종료되고 공간이 남았을 때 새로운 프로세스가 어떤 위치에 올릴지 결정하는 문제가 생김 -> "동적 메모리 할당 문제"

    - 최초 적합: 위쪽이나 아래쪽에서 시작해서 공간이 있다면 바로 할당 (시간적 이득)
    - 최적 적합: 프로세스의 크기 이상인 공간 중 가장 작은 공간부터 할당 (공간적 이득)
    - 최악 적합: 프로세스의 크기와 공간이 가장 많은 차이가 일어나는 공간부터 할당 (비효율)


## 불연속 할당
메모리가 연속적으로 할당하지 않는 방법으로 페이징 기법이 있다.

- 페이징: 동일한 페이지 단위로 나누어 메모리의 서로 다른 위치에 프로세스를 할당<br>
    - 공간은 균일하지만 주소 변환이 복잡해짐 
    - 페이지 테이블, 그리고 실제 주소 메모리 참조까지 총 두번의 메모리 접근이 일어나기 때문에 주소변환이 복잡하고 번거롭다 (TLB로 해결)
    ![image](https://github.com/cJinu/CS/assets/94429120/99537fc7-f061-4ab5-a10b-2e5c2693b22b)
    - 외부 단편화는 일어날 수 없지만 내부 단편화는 일어날 수 있다 (마지막 페이지에 한해서)

- 세그멘테이션
    - 페이지 단위가 아닌 의미 단위인 세그먼트로 나누는 방식
    - 프로세스를 나누는 메모리는 코드, 데이터, 스택, 힙 영역으로 이루어진다.
    - 코드와 데이터를 나누거나 코드 내의 작은 함수를 세그먼트로 분류하여 메모리에 할당 가능하다
    - 공유와 보안 측면에서는 용이하지만 메모리의 공간의 균일하지 않다.

    ![image](https://github.com/cJinu/CS/assets/94429120/eebc0074-db04-47d0-aa29-befe87f22b74)
    
    할당 방식은 페이징과 동일하지만 이때는 Segment Table을 이용하여 할당한다. <br>
    이때 세그먼트 테이블을 보면 할당되는 공간이 모두 다른것을 알 수 있다. <br>
    외부 단편화는 일어나지만 내부 단편화는 일어날 수 없다. <br>


# 페이지 교체 알고리즘
메모리는 한정되어 있기 때문에 스와핑이 많이 일어난다. <br>
페이지 교체 알고리즘은 스와핑이 많이 일어나지 않도록 설계되어야하며 이를 기반으로 페이지 교체 알고리즘이 설계 되었다.

### FIFO(First In First Out)
![FIFO](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FU5nSm%2Fbtq9PUpYAl8%2FWtyueVDWhp6E2nLGbNHYWK%2Fimg.png)

- 가장 먼저온 페이지를 교체 영역에서 가장 먼저 놓는 방법
- 간단하고, 초기화 코드에 대해 적절한 방법
- 들어온 시간을 저장하거나 올라온 순서를 큐에 저장.
- 직관적으로 생각할 때 프레임의 수가 많아질수록 페이지 결함의 횟수는 감소함
- 실제로 그렇지 않게 되는 현상이 나타날 수 있다.

### LRU (Last Recently Used)
![LRU](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb1csvE%2Fbtq9IZlYnx0%2FwbSZzZfsBkbmQ80nnf9LdK%2Fimg.png)
- 참조가 가장 오래된 페이지를 변경
- 시간 지역성(temporal locality)성질 고려함.(최근에 참조된 페이지가 가까운 미래에 다시 참조될 가능성이 높은 성질)
- 사용된 시간을 알수있는 부분을 저장하여 가장 오랫동안 참조되지 않는 데이터를 제거(페이지마다 카운터 필요)
- 큐로 구현가능. 사용한 데이터를 큐에서 제거하여 맨 위로다시 올리고, 프레임이 모자랄 경우 맨 아래에 있는 데이터를 삭제
- 단점: 프로세스가 주기억장치에 접근할때마다 참조된 페이지 시간을 기록해야 하므로 막대한 오버헤드가 발생 <br>카운터나 큐, 스택과 같은 별도의 하드웨어가 필요

### NUR (Not Used Recently)
LRU에서 발전한 알고리즘 clock 알고리즘이라고도 불린다. <br>
최근에 사용하지 않은 페이지 교체 (LRU를 근사한 알고리즘)
![](https://blog.kakaocdn.net/dn/bFLfwl/btrmGg7P1GN/RhZ8a263lfv7OxMiWytbw1/img.png)

- 교체되는 페이지의 참조 시점이 가장 오래되었다는 것을 보장하지는 못함
- 적은 오버헤드로 적절한 성능
- 동일 그룹 내에서 선택 무작위
- 각 페이지마다 두개의 비트 참조 비트(Reference Bit)와 변형 비트(Modified Bit, Birty Bit)가 사용됨
- 참조 비트: 페이지가 참조되지 않았을 때 0, 호출되었을 때 1 (모든 참조비트를 주기적으로 0으로 변경)
- 변형 비트: 페이지 내용이 변경되지 않았을 때는 0, 변경되었을 때 1
- 우선순위: 참조비트 > 변형비트
### LFU (Least Frequently Used)
- 가장 참조가 적은 페이지를 교체
- 단점: 가장 최근에 불러온 페이지가 교체되는 경우 발생 <br>
        구현은 더 복잡하며 막대한 오버헤드 발생
